
@article{abadiTensorFlowLargeScaleMachine,
  title = {{{TensorFlow}}: {{Large-Scale Machine Learning}} on {{Heterogeneous Distributed Systems}}},
  author = {Abadi, Mart{\i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  pages = {19},
  langid = {english},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\8TDVLVTP\\Abadi et al. - TensorFlow Large-Scale Machine Learning on Hetero.pdf}
}

@book{aggarwalNeuralNetworksDeep2018,
  title = {Neural {{Networks}} and {{Deep Learning}}: {{A Textbook}}},
  shorttitle = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Aggarwal, Charu C.},
  year = {2018},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-94463-0},
  isbn = {978-3-319-94462-3 978-3-319-94463-0},
  langid = {english},
  keywords = {Adam,autoencoder,backpropagation,conjugate gradient-descent,Convolutional Neural Networks,Deep Learning,deep reinforcement learning,dropout,generative adversarial networks,Kohonean self-organizaing map,logistic regression,Machine Learning,Neural networks,perceptron,pretraining,Radial Basis Function Networks,Recurrent Neural Networks,Restricted Boltzmann Machines,RMSProp,word2vec},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\MJ7H5CDG\\Aggarwal - 2018 - Neural Networks and Deep Learning A Textbook.pdf}
}

@article{bengioLearningLongtermDependencies1994,
  title = {Learning Long-Term Dependencies with Gradient Descent Is Difficult},
  author = {Bengio, Y. and Simard, P. and Frasconi, P.},
  year = {1994},
  month = mar,
  journal = {IEEE Transactions on Neural Networks},
  volume = {5},
  number = {2},
  pages = {157--166},
  issn = {1941-0093},
  doi = {10.1109/72.279181},
  abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.{$<>$}},
  keywords = {Computer networks,Cost function,Delay effects,Discrete transforms,Displays,Intelligent networks,Neural networks,Neurofeedback,Production,Recurrent neural networks},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\7B4EZEG2\\Bengio et al. - 1994 - Learning long-term dependencies with gradient desc.pdf;C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\LB8DLN9P\\279181.html}
}

@article{briggmanImagingDedicatedMultifunctional2006,
  title = {Imaging {{Dedicated}} and {{Multifunctional Neural Circuits Generating Distinct Behaviors}}},
  author = {Briggman, Kevin L. and Kristan, William B.},
  year = {2006},
  month = oct,
  journal = {J. Neurosci.},
  volume = {26},
  number = {42},
  pages = {10925--10933},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3265-06.2006},
  abstract = {Central pattern generators (CPGs) control both swimming and crawling in the medicinal leech. To investigate whether the neurons comprising these two CPGs are dedicated or multifunctional, we used voltage-sensitive dye imaging to record from {$\sim$}80\% of the {$\sim$}400 neurons in a segmental ganglion. By eliciting swimming and crawling in the same preparation, we were able to identify neurons that participated in either of the two rhythms, or both. More than twice as many cells oscillated in-phase with crawling (188) compared with swimming (90). Surprisingly, 84 of the cells (93\%) that oscillated with swimming also oscillated with crawling. We then characterized two previously unidentified interneurons, cells 255 and 257, that had interesting activity patterns based on the imaging results. Cell 255 proved to be a multifunctional interneuron that oscillates with and can perturb both rhythms, whereas cell 257 is an interneuron dedicated to crawling. These results show that the swimming and crawling networks are driven by both multifunctional and dedicated circuitry.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2006 Society for Neuroscience 0270-6474/06/2610925-09\$15.00/0},
  langid = {english},
  pmid = {17050731},
  keywords = {central pattern generator,dedicated circuitry,live-cell imaging,motor control,multifunctional circuitry,single-trial analysis},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\HVCEF5DZ\\Briggman and Kristan - 2006 - Imaging Dedicated and Multifunctional Neural Circu.pdf;C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\RBLGMC6S\\10925.html}
}

@misc{chollet2015keras,
  title = {Keras},
  author = {Chollet, Francois and others},
  year = {2015},
  publisher = {{GitHub}},
  howpublished = {https://github.com/fchollet/keras}
}

@article{dumorEstimatingChinaTrade2019,
  title = {Estimating {{China}}'s {{Trade}} with {{Its Partner Countries}} within the {{Belt}} and {{Road Initiative Using Neural Network Analysis}}},
  author = {Dumor, Koffi and Li, Yao},
  year = {2019},
  month = mar,
  journal = {Sustainability},
  volume = {11},
  pages = {1449},
  doi = {10.3390/su11051449},
  abstract = {The Belt and Road Initiative (BRI) under the auspices of the Chinese government was created as a regional integration and development model between China and her trade partners. Arguments have been raised as to whether this initiative will be beneficial to participating countries in the long run. We set to examine how to estimate this trade initiative by comparing the relative estimation powers of the traditional gravity model with the neural network analysis using detailed bilateral trade exports data from 1990 to 2017. The results show that neural networks are better than the gravity model approach in learning and clarifying international trade estimation. The neural networks with fixed country effects showed a more accurate estimation compared to a baseline model with country-year fixed effects, as in the OLS estimator and Poisson pseudo-maximum likelihood. On the other hand, the analysis indicated that more than 50\% of the 6 participating East African countries in the BRI were able to attain their predicted targets. Kenya achieved an 80\% (4 of 5) target. Drawing from the lessons of the BRI and the use of neural network model, it will serve as an important reference point by which other international trade interventions could be measured and compared.},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\Q4CADKGC\\Dumor and Li - 2019 - Estimating Chinaâ€™s Trade with Its Partner Countrie.pdf;C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\QU9EV339\\The-Fully-Connected-Neural-Network-model-used-in-our-paper_fig3_331627739.html}
}

@article{elmanFindingStructureTime1990,
  title = {Finding {{Structure}} in {{Time}}},
  author = {Elman, Jeffrey L.},
  year = {1990},
  journal = {Cognitive Science},
  volume = {14},
  number = {2},
  pages = {179--211},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog1402_1},
  abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402\_1},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\UCYAXJCQ\\Elman - 1990 - Finding Structure in Time.pdf;C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\RA39BRNB\\s15516709cog1402_1.html}
}

@article{eschEvidenceSequentialDecision2002,
  title = {Evidence for {{Sequential Decision Making}} in the {{Medicinal Leech}}},
  author = {Esch, Teresa and Mesce, Karen A. and Kristan, William B.},
  year = {2002},
  month = dec,
  journal = {J. Neurosci.},
  volume = {22},
  number = {24},
  pages = {11045--11054},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.22-24-11045.2002},
  abstract = {Decision making can be a complex task involving a sequence of subdecisions. For example, we decide to pursue a goal (e.g., get something to eat), then decide how to accomplish that goal (e.g., go to a restaurant), and then make a sequence of more specific plans (e.g., which restaurant to go to, how to get there, what to order, etc.). In characterizing the effects of stimulating individual brain neurons in the isolated nervous system of the leech Hirudo medicinalis, we have found evidence that leeches also make decisions sequentially. In this study, we describe a pair of interneurons that elicited locomotory motor programs, either swimming or crawling, in isolated nerve cords. In semi-intact animals, stimulating the same neurons also produced either swimming or crawling, and which behavior was produced could be controlled experimentally by manipulating the depth of saline around the intact part of the leech. These same neurons were excited and fired strongly when swimming or crawling occurred spontaneously or in response to mechanosensory stimulation. We conclude that these brain interneurons help to decide on locomotion (i.e., they are ``locomotory command-like neurons'') and that the ultimate behavior is determined downstream, in a part of the decision-making hierarchy that monitors stimuli related to the depth of fluid surrounding the leech.},
  chapter = {ARTICLE},
  copyright = {Copyright \textcopyright{} 2002 Society for Neuroscience},
  langid = {english},
  pmid = {12486200},
  keywords = {choice behavior,leeches,locomotion,motor patterns,multifunctional neurons,neural circuits},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\QBQM25AE\\Esch et al. - 2002 - Evidence for Sequential Decision Making in the Med.pdf;C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\P7NC6CXW\\11045.html}
}

@article{flynnMultifunctionalityReservoirComputer2021a,
  title = {Multifunctionality in a Reservoir Computer},
  author = {Flynn, Andrew and Tsachouridis, Vassilios A. and Amann, Andreas},
  year = {2021},
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {31},
  number = {1},
  pages = {013125},
  publisher = {{AIP Publishing LLC}},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\ZHVNMTHW\\Multifunctionality in a reservoir computer Chaos.pdf;C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\DH7RHCAD\\5.html}
}

@article{gersLearningForgetContinual2000,
  title = {Learning to Forget: {{Continual}} Prediction with {{LSTM}}},
  shorttitle = {Learning to Forget},
  author = {Gers, Felix A. and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  year = {2000},
  journal = {Neural computation},
  volume = {12},
  number = {10},
  pages = {2451--2471},
  publisher = {{MIT Press}},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\IAKT4S6B\\Gers et al. - 2000 - Learning to forget Continual prediction with LSTM.pdf;C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\9ASTJ645\\6789445.html}
}

@article{gettingEmergingPrinciplesGoverning1989,
  title = {Emerging Principles Governing the Operation of Neural Networks},
  author = {Getting, Peter A.},
  year = {1989},
  journal = {Annual review of neuroscience},
  volume = {12},
  number = {1},
  pages = {185--204},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\HN3GAH3R\\Emerging Principles Governing the Operation of Neu.pdf}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\5IYRYEBL\\Hochreiter and Schmidhuber - 1997 - Long Short-Term Memory.pdf;C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\P9V3EQT6\\6795963.html}
}

@techreport{jordanSerialOrderParallel1986,
  title = {Serial Order: A Parallel Distributed Processing Approach. {{Technical}} Report, {{June}} 1985-{{March}} 1986},
  shorttitle = {Serial Order},
  author = {Jordan, M. I.},
  year = {1986},
  month = may,
  number = {AD-A-173989/5/XAB; ICS-8604},
  institution = {{California Univ., San Diego, La Jolla (USA). Inst. for Cognitive Science}},
  abstract = {A theory of serial order is proposed that attempts to deal both with the classical problem of the temporal organization of internally generated action sequences as well as with certain of the parallel aspects of sequential behavior. The theory describes a dynamical system that is embodied as a parallel distributed processing or connectionist network. The trajectories of this dynamical system come to follow desired paths corresponding to particular action sequences as a result of a learning process during which constraints are imposed on the system. These constraints enforce sequentiality where necessary and, as they are relaxed, performance becomes more parallel. The theory is applied to the problem of coarticulation in speech production and simulation experiments are presented.},
  langid = {english},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\WPMY9AJ7\\6910294.html}
}

@article{kriegeskorteNeuralNetworkModels2019,
  title = {Neural Network Models and Deep Learning},
  author = {Kriegeskorte, Nikolaus and Golan, Tal},
  year = {2019},
  month = apr,
  journal = {Current Biology},
  volume = {29},
  number = {7},
  pages = {R231-R236},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2019.02.034},
  abstract = {Originally inspired by neurobiology, deep neural network models have become a powerful tool of machine learning and artificial intelligence. They can approximate functions and dynamics by learning from examples. Here we give a brief introduction to neural network models and deep learning for biologists. We introduce feedforward and recurrent networks and explain the expressive power of this modeling framework and the backpropagation algorithm for setting the parameters. Finally, we consider how deep neural network models might help us understand brain computation.},
  langid = {english},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\LLHJAK8C\\Kriegeskorte and Golan - 2019 - Neural network models and deep learning.pdf;C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\6KENCQVI\\S0960982219302040.html}
}

@article{medskerRECURRENTNEURALNETWORKS,
  title = {{{RECURRENT NEURAL NETWORKS}}},
  author = {Medsker, L. R. and Jain, L. C.},
  publisher = {{Citeseer}},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\U6FH9LVK\\Medsker and Jain - RECURRENT NEURAL NETWORKS.pdf}
}

@techreport{rumelhartLearningInternalRepresentations1985,
  title = {Learning Internal Representations by Error Propagation},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1985},
  institution = {{California Univ San Diego La Jolla Inst for Cognitive Science}},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\H4N7PKJ2\\Rumelhart et al. - 1985 - Learning internal representations by error propaga.pdf;C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\7M7D5MMG\\ADA164453.html}
}

@article{scarselliUniversalApproximationUsing1998,
  title = {Universal {{Approximation Using Feedforward Neural Networks}}: {{A Survey}} of {{Some Existing Methods}}, and {{Some New Results}}},
  shorttitle = {Universal {{Approximation Using Feedforward Neural Networks}}},
  author = {Scarselli, Franco and Chung Tsoi, Ah},
  year = {1998},
  month = jan,
  journal = {Neural Networks},
  volume = {11},
  number = {1},
  pages = {15--37},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(97)00097-X},
  abstract = {In this paper, we present a review of some recent works on approximation by feedforward neural networks. A particular emphasis is placed on the computational aspects of the problem, i.e. we discuss the possibility of realizing a feedforward neural network which achieves a prescribed degree of accuracy of approximation, and the determination of the number of hidden layer neurons required to achieve this accuracy. Furthermore, a unifying framework is introduced to understand existing approaches to investigate the universal approximation problem using feedforward neural networks. Some new results are also presented. Finally, two training algorithms are introduced which can determine the weights of feedforward neural networks, with sigmoidal activation neurons, to any degree of prescribed accuracy. These training algorithms are designed so that they do not suffer from the problems of local minima which commonly affect neural network learning algorithms.},
  langid = {english},
  keywords = {Approximation by neural networks,Approximation of polynomials,Constructive approximation,Feedforward neural networks,Multilayer neural networks,Radial basis functions,Universal approximation},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\M4A4TE5N\\Scarselli and Chung Tsoi - 1998 - Universal Approximation Using Feedforward Neural N.pdf;C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\UHYNIDPN\\S089360809700097X.html}
}

@misc{teamKerasDocumentationLSTM,
  title = {Keras Documentation: {{LSTM}} Layer},
  shorttitle = {Keras Documentation},
  author = {Team, Keras},
  abstract = {Keras documentation},
  howpublished = {https://keras.io/api/layers/recurrent\_layers/lstm/},
  langid = {english},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\7SLVNEME\\lstm.html}
}

@article{vanhoudtReviewLongShortTerm2020,
  title = {A {{Review}} on the {{Long Short-Term Memory Model}}},
  author = {Van Houdt, Greg and Mosquera, Carlos and N{\'a}poles, Gonzalo},
  year = {2020},
  month = dec,
  journal = {Artificial Intelligence Review},
  volume = {53},
  doi = {10.1007/s10462-020-09838-1},
  abstract = {Long Short-Term Memory (LSTM) has transformed both machine learning and neurocomputing fields. According to several online sources, this model has improved Google's speech recognition, greatly improved machine translations on Google Translate, and the answers of Amazon's Alexa. This neural system is also employed by Facebook, reaching over 4 billion LSTM-based translations per day as of 2017. Interestingly, recurrent neural networks had shown a rather discrete performance until LSTM showed up. One reason for the success of this recurrent network lies in its ability to handle the exploding / vanishing gradient problem, which stands as a difficult issue to be circumvented when training recurrent or very deep neural networks. In this paper, we present a comprehensive review that covers LSTM's formulation and training, relevant applications reported in the literature and code resources implementing this model for a toy example.},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\ZRYGIDB9\\Van Houdt et al. - 2020 - A Review on the Long Short-Term Memory Model.pdf}
}

@article{yuReviewRecurrentNeural2019,
  title = {A {{Review}} of {{Recurrent Neural Networks}}: {{LSTM Cells}} and {{Network Architectures}}},
  shorttitle = {A {{Review}} of {{Recurrent Neural Networks}}},
  author = {Yu, Yong and Si, Xiaosheng and Hu, Changhua and Zhang, Jianxun},
  year = {2019},
  month = jul,
  journal = {Neural Computation},
  volume = {31},
  number = {7},
  pages = {1235--1270},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01199},
  abstract = {Recurrent neural networks (RNNs) have been widely adopted in research areas concerned with sequential data, such as text, audio, and video. However, RNNs consisting of sigma cells or tanh cells are unable to learn the relevant information of input data when the input gap is large. By introducing gate functions into the cell structure, the long short-term memory (LSTM) could handle the problem of long-term dependencies well. Since its introduction, almost all the exciting results based on RNNs have been achieved by the LSTM. The LSTM has become the focus of deep learning. We review the LSTM cell and its variants to explore the learning capacity of the LSTM cell. Furthermore, the LSTM networks are divided into two broad categories: LSTM-dominated networks and integrated LSTM networks. In addition, their various applications are discussed. Finally, future research directions are presented for LSTM networks.},
  file = {C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\UCXTWFTI\\Yu et al. - 2019 - A Review of Recurrent Neural Networks LSTM Cells .pdf;C\:\\Users\\lekoi\\Documents\\AM6018\\Zotero\\storage\\B8RRKQEX\\8737887.html}
}


